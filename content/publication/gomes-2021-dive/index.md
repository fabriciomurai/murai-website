---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Sequence-Based Word Embeddings for Effective Text Classification
subtitle: ''
summary: ''
authors:
- Bruno Guilherme Gomes
- Fabricio Murai
- Olga Goussevskaia
- Ana Paula da Silva
tags: []
categories: []
date: '2021-01-01'
lastmod: 2021-08-11T15:11:29-03:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-08-11T18:11:28.792564Z'
publication_types:
- '1'
abstract: In this work we present DiVe (Distance-based Vector Embedding), a new word
  embedding technique based on the Logistic Markov Embedding (LME). First, we generalize
  LME to consider different distance metrics and address existing scalability issues
  using negative sampling, thus making DiVe scalable for large datasets. In order
  to evaluate the quality of word embeddings produced by DiVe, we used them to train
  standard machine learning classifiers, with the goal of performing different Natural
  Language Processing (NLP) tasks. Our experiments demonstrated that DiVe is able
  to outperform existing (more complex) machine learning approaches, while preserving
  simplicity and scalability.
publication: '*Natural Language Processing and Information Systems*'
doi: 10.1007/978-3-030-80599-9_12
---
