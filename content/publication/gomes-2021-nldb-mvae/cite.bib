@inproceedings{gomes2021nldb-mvae,
 abstract = {Variational autoencoders (VAEs) have been successfully used to learn good representations in unsupervised settings, especially for image data. More recently, mixture variational autoencoders (MVAEs) have been proposed to enhance the representation capabilities of VAEs by assuming that data can come from a mixture distribution. In this work, we adapt MVAEs for text processing by modeling each component's joint distribution of latent variables and document's bag-of-words as a graphical model known as the Boltzmann Machine, popular in natural language processing for performing well in a number of tasks. The proposed model, MVAE-BM, can learn text representations from unlabeled data without requiring pre-trained word embeddings. We evaluate the representations obtained by MVAE-BM on six corpora w.r.t. the perplexity metric and accuracy on binary and multi-class text classification. Despite its simplicity, our results show that MVAE-BM's performance is on par with or superior to that of modern deep learning techniques such as BERT and RoBERTa. Last, we show that the mapping to mixture components learned by the model lends itself naturally to document clustering.},
 address = {Cham},
 annote = {n̆derlineQualis: B1; H5-index: 13;},
 author = {Guilherme Gomes, Bruno and Murai, Fabricio and Goussevskaia, Olga and da Silva, Ana Paula},
 booktitle = {Natural Language Processing and Information Systems},
 doi = {10.1007/978-3-030-80599-9_5},
 editor = {Métais, Elisabeth and Meziane, Farid and Horacek, Helmut and Kapetanios, Epaminondas},
 isbn = {978-3-030-80599-9},
 pages = {46--56},
 publisher = {Springer International Publishing},
 title = {Mixture Variational Autoencoder of Boltzmann Machines for Text Processing},
 year = {2021}
}

